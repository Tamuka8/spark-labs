<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<p><a href="../README.md">&lt;&lt; back to main index</a></p>
<h1 id="lab-3.1-rdd-basics-operations">Lab 3.1 : RDD Basics operations</h1>
<h3 id="overview">Overview</h3>
<ul>
<li>Learning basic operations like filter / map / count</li>
<li>work with larger sized RDDs</li>
<li>Load multiple files into a single RDD</li>
<li>Save computed RDDs</li>
</ul>
<h3 id="depends-on">Depends On</h3>
<p>None</p>
<h3 id="run-time">Run time</h3>
<p>30-40 mins</p>
<h2 id="step-1-basic-rdd-operations">STEP 1: Basic RDD Operations</h2>
<h4 id="in-standalone-env.">In Standalone env.</h4>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" data-line-number="1">    $   <span class="ex">~/spark/bin/spark-shell</span></a></code></pre></div>
<h4 id="in-hadoop-env">In Hadoop env</h4>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" data-line-number="1">    $  <span class="ex">spark-shell</span></a></code></pre></div>
<h3 id="controlling-the-ui-options">Controlling the UI options</h3>
<p>Spark Shell by default publishes a UI on port number 4040.<br />
How ever when multiple apps are running, and port 4040 already taken, Spark Shell will try to find an open port (4041, 4042 ..etc)</p>
<p>Specifying a custom port</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb3-1" data-line-number="1">    $   <span class="ex">spark-shell</span>  --conf spark.ui.port=4060</a></code></pre></div>
<p>Turn off UI altogether</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" data-line-number="1">    $   <span class="ex">spark-shell</span>  --conf spark.ui.enabled=false</a></code></pre></div>
<h2 id="step-2-load-a-small-text-file">Step 2: Load a small text file</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb5-1" data-line-number="1"></a>
<a class="sourceLine" id="cb5-2" data-line-number="2">scala&gt;</a>
<a class="sourceLine" id="cb5-3" data-line-number="3">    <span class="kw">val</span> f = sc.<span class="fu">textFile</span>(<span class="st">&quot;/data/text/twinkle/sample.txt&quot;</span>)</a></code></pre></div>
<p><strong>=&gt; what is the ‘type’ of f ?</strong><br />
Hint : just type <code>f</code> in the shell Here is a possible output</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb6-1" data-line-number="1">    scala&gt; f</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">    res0: org.<span class="fu">apache</span>.<span class="fu">spark</span>.<span class="fu">rdd</span>.<span class="fu">RDD</span>[String] = /data/text/twinkle/sample.<span class="fu">txt</span> MappedRDD[<span class="dv">3</span>] at textFile at &lt;console&gt;:<span class="dv">12</span></a></code></pre></div>
<h2 id="step-3-filter">STEP 3: Filter</h2>
<p>Let’s find how many lines contain the word ‘twinkle’.<br />
We will use the ‘filter’ function</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb7-1" data-line-number="1">    <span class="kw">val</span> filtered = f.<span class="fu">filter</span>(line =&gt; line.<span class="fu">contains</span>(<span class="st">&quot;twinkle&quot;</span>))</a>
<a class="sourceLine" id="cb7-2" data-line-number="2"></a>
<a class="sourceLine" id="cb7-3" data-line-number="3">    <span class="co">// short hand version</span></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    <span class="kw">val</span> filtered = f.<span class="fu">filter</span>(_.<span class="fu">contains</span>(<span class="st">&quot;twinkle&quot;</span>))</a></code></pre></div>
<p>After entering the above in Spark-shell…<br />
<strong>=&gt; Goto Spark shell UI (port number 4040)</strong><br />
<strong>=&gt; Inspect the ‘Stages’ section in the UI.</strong><br />
<strong>=&gt; How is the filter executed? Can you explain the behavior?</strong></p>
<p><strong>=&gt; Count how many lines contain the word ‘twinkle’</strong><br />
hint : apply <code>count()</code> to <code>filtered</code> variable</p>
<p>Here is a sample output</p>
<pre class="console"><code>    15/03/31 23:19:30 INFO DAGScheduler: Stage 0 (count at &lt;console&gt;:17) finished in 0.074 s
    15/03/31 23:19:30 INFO DAGScheduler: Job 0 finished: count at &lt;console&gt;:17, took 0.141676 s
    res1: Long = 2  &lt;--- this is the result of count()</code></pre>
<p><strong>=&gt; Check the Stages in UI, what do you see?</strong><br />
<strong>=&gt; How long did the job take?</strong><br />
<strong>=&gt; Print out all the lines containing the word ‘twinkle’</strong><br />
Hint : <code>collect()</code></p>
<p>Here is a sample output</p>
<pre class="console"><code>res2: Array[String] = Array(twinkle twinkle little star, twinkle twinkle little star)</code></pre>
<p><strong>=&gt; Checkout ‘DAG’ visualization</strong></p>
<p><img src="../assets/images/3.1c.png" style="border: 5px solid grey; max-width:100%;"/></p>
<p><strong>=&gt; Quit Spark-shell using ‘exit’ or pressing Control+D</strong></p>
<h2 id="step-4-large-data-sets">STEP 4: Large data sets</h2>
<p><strong>==&gt; Quit previous spark-shell session, if you haven’t done so yet.. <code>Control + D</code></strong></p>
<p>We have some large data sets of ‘twinkle’ data generated in <code>/data/text/twinkle</code> directory.</p>
<p><strong>=&gt; Verify the data files and their sizes by doing the following</strong><br />
If you are on a standalone Spark environment:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb10-1" data-line-number="1">    $   <span class="ex">l</span> /data/text/twinkle</a></code></pre></div>
<p>If you are on Hadoop Spark environment:</p>
<pre><code>  $ hdfs dfs -ls /data/text/twinkle/</code></pre>
<p>Your output might look like this</p>
<p><img src="../assets/images/3.1a.png" style="border: 5px solid grey; max-width:100%;"/></p>
<p>We are going to use these files in spark</p>
<p><strong>=&gt; [Optional] You can use the following script to generate more data if you need to</strong></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb12-1" data-line-number="1">    $   <span class="bu">cd</span>  ~/data/text/twinkle</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">    $   <span class="ex">./create-data-files.sh</span></a></code></pre></div>
<p>This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)</p>
<h2 id="step-5-start-shell-with-more-memory">STEP 5: Start shell With More Memory</h2>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb13-1" data-line-number="1">    $   <span class="ex">~/spark/bin/spark-shell</span>  --executor-memory 1G  --driver-memory 1G</a></code></pre></div>
<h2 id="step-6-process-a-large-file">STEP 6: Process a large file</h2>
<p><strong>=&gt; In Spark Shell, load <code>/data/text/twinkle/100M.data</code></strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb14-1" data-line-number="1">scala&gt;</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">    <span class="kw">val</span> f = sc.<span class="fu">textFile</span>(<span class="st">&quot;/data/text/twinkle/100M.data&quot;</span>)</a></code></pre></div>
<p><strong>=&gt; Count number of lines that have the word “diamond”</strong><br />
hint : <code>filter</code> and <code>count</code></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb15-1" data-line-number="1">    <span class="kw">val</span> filtered = f.<span class="fu">filter</span>(_.<span class="fu">contains</span>(<span class="st">&quot;?? what is the word ??&quot;</span>))</a></code></pre></div>
<p><strong>=&gt; How many ‘tasks’ are used in the above calculation</strong><br />
hint : Check spark shell UI at port 4040 <img src="../assets/images/3.1b.png" style="border: 5px solid grey; max-width:100%;" /></p>
<p><strong>=&gt; Can you explain the number of tasks?</strong><br />
Hint : check number of partitions in RDD using</p>
<pre><code>    f.getNumPartitions</code></pre>
<p><strong>=&gt; Count number of lines that does NOT have the word ‘diamond’</strong><br />
Hint : use negative operator !<br />
<code>filter(line =&gt; ! line.contains(&quot;diamond&quot;))</code></p>
<p><strong>=&gt; Verify both counts add up to the total line count</strong></p>
<p><strong>=&gt; Notice the time taken for each operation</strong></p>
<p><strong>=&gt; Try the above with larger data files : 500M.data … 1G.data</strong> - note the times taken - how many tasks?</p>
<h2 id="step-7-loading-multiple-files">STEP 7: Loading multiple files</h2>
<p>**=&gt; Load all *.data files under /data/text/twinkle directory**<br />
hint : <code>val allData = sc.textFile(&quot;/data/text/twinkle/*.data&quot;)</code></p>
<p><strong>=&gt; Do a count() on RDD.</strong> Notice the partition count and time taken to execute Verify partition count from Spark-Shell UI</p>
<h2 id="step-8-saving-the-rdd">STEP 8: Saving the RDD</h2>
<p>Continuing with the big RDD created on step (7)….</p>
<p><strong>=&gt; Create a new RDD by filtering first RDD for word ‘diamond’</strong><br />
Hint : <code>filter</code></p>
<p><strong>=&gt; Save the new RDD into a directory</strong><br />
Hint : <code>rdd.saveAsTextFile(&quot;MYNAME_out1&quot;)</code> (Change MY_NAME accordingly)</p>
<p><strong>=&gt; Inspect the console output during the run.</strong></p>
<p><strong>=&gt; Inspect the output directory</strong><br />
In standalone Spark env,</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb17-1" data-line-number="1">    $   <span class="fu">ls</span> -lh   MY_NAME_out1  # change MY_NAME accordingly</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">    $   <span class="fu">less</span>  MYNAME_out1/part-00000</a></code></pre></div>
<p>In Hadoop Spark Env:</p>
<pre><code>  $  hdfs dfs -ls  MYNAME_out1</code></pre>
<p>Or use HDFS File Browser</p>
<p><strong>=&gt; What do you see as output?</strong></p>
<h2 id="bonus-lab-merging-partitions-into-a-single-one">Bonus Lab: Merging partitions into a single one</h2>
<p>When we saved data in the above section, there are multiple files created in output directory. Can you just create one output file?<br />
Hint : see the API for <code>coalesce or repartition</code></p>
