<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

Lab 3.1 : RDD Basics operations
================================

- [Standalone version](3.1-rdd-basics.md) 
- [Hadoop version](3.1H-rdd-hadoop.md)

### Overview
* Learning basic operations like filter / map / count
* work with larger sized RDDs
* Load multiple files into a single RDD
* Save computed RDDs



### Depends On
None

### Run time
30-40 mins


## STEP 1:  Fire up Spark shell

```bash
    $   cd   #  switch to home dir
    $   ~/spark/bin/spark-shell
```


## STEP 2: Load a simple text file

```scala
scala>
    val f = spark.read.textFile("data/twinkle/sample.txt")

    // v1.6
    //val f = sc.textFile("data/twinkle/sample.txt")
```


**=> what is the 'type' of f ?**  
Hint : just type `f` in the shell
Here is a possible output

```scala
    scala> f
    f: org.apache.spark.sql.Dataset[String] = [value: string]
```


## STEP 3: Filter
Let's find how many lines contain the word 'twinkle'
We will use the 'filter' function

```scala
    val filtered = f.filter(line => line.contains("twinkle"))

    // short hand version
    //val filtered = f.filter(_.contains("twinkle"))
```

After entering the above in Spark-shell...  
**=> Goto Spark shell UI **  
**=> Inspect the 'Stages' section in the UI.**  
**=> How is the filter executed? Can you explain the behavior?**  

**=> Count how many lines contain the word 'twinkle'**  
hint : apply `count()` to `filtered` variable

Here is a sample output

```console
15/03/31 23:19:30 INFO DAGScheduler: Stage 0 (count at <console>:17) finished in 0.074 s
15/03/31 23:19:30 INFO DAGScheduler: Job 0 finished: count at <console>:17, took 0.141676 s
res1: Long = 2  <--- this is the result of count()
```


**=> Check the Stages in UI,  what do you see?**  
**=> How long did the job take?**  
**=> Print out all the lines containing the word 'twinkle'**   
Hint : `collect()`  
Here is a sample output
```console
res2: Array[String] = Array(twinkle twinkle little star, twinkle twinkle little star)
```

**=> Checkout 'DAG' visualization**

<img src="../images/3.1c.png" style="border: 5px solid grey; max-width:100%;"/>

**=> Quit Spark-shell using 'exit'  or pressing  Control+D**


## STEP 4:  Large data sets
**==> Quit previous spark-shell session, if you haven't done so yet.. `Control + D`**  

We have some large data sets of 'twinkle' data generated in `~/data/twinkle`  directory.

**=> Verify the data files and their sizes by doing a**
```
    $   l ~/data/twinkle
```
Your output might look like this

<img src="../images/3.1a.png" style="border: 5px solid grey; max-width:100%;"/>

We are going to use these files in spark

**=> [Optional] You can use the following script to generate more data if you need to**  

    $   cd  ~/spark-labs/data/twinkle
    $   ./create-data-files.sh


This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)

## STEP 5:  Start shell With More Memory

```bash
    $   ~/spark/bin/spark-shell  --executor-memory 1G  --driver-memory 1G
```

## STEP 6: Process a large file
**=> In Spark Shell, load `data/twinkle/100M.data`**  
```scala
scala>
    val f = spark.read.textFile("data/twinkle/100M.data")
```

**=> Count number of lines that have the word "diamond"**  
hint : `filter`  and `count`

**=> How many 'tasks' are used in the above calculation**  
Hint : Check spark shell UI

<img src="../images/3.1b.png" style="border: 5px solid grey; max-width:100%;" />

**=> Can you explain the number of tasks?**  
Hint : check number of partitions in RDD using `getNumPartitions`  or `partitions.length`  
```
    f.rdd.getNumPartitions (2.1),  f.getNumPartitions(1.6)
```


**=> Count number of lines that does NOT have the word 'diamond'**  
Hint : use negative operator  !  
`f.filter(line => ! line.contains("diamond")) `

**=> Verify both counts add up to the total line count**

**=> Notice the time taken for each operation**

**=> Try the above with larger data files : 500M.data  ... 1G.data**
  - note the times taken
  - how many tasks?

## STEP 7: Loading multiple files
**=> Load all *.data files under  data/twinkle  directory**  
```scala
    val f = spark.read.textFile("data/twinkle/*.data")
```

**=> Do a count() on RDD.**  
Notice the partition count and time taken to execute
Verify partition count from Spark-Shell UI

## STEP 8:  Saving the RDD
Continuing with the big RDD created on step (5)....

**=> Create a new RDD by filtering first RDD for word 'diamond'**  

```scala
    val filtered = f.filter(_.contains("diamond"))
```

**=> Save the new RDD into a directory**  
```scala
    filtered.saveAsTextFile("out1")
```

**=> Inspect the console output during the run.**

Inspect the output directory
You can use

    $    ls -lh   out1


To see files created use :

    $   less  out1/part-00000


**=> What do you see as output?**


## Bonus Lab: Merging partitions into a single one
When we saved data in the above section, there are multiple files created in output directory.   Can you just create one output file?   
Hint : see the API for `coalesce or repartition`
