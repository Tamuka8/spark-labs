{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel='stylesheet' href='../assets/css/main.css'/>\n",
    "\n",
    "[<< back to main index](../README.md)\n",
    "\n",
    "# Lab 4.3 : Dataset\n",
    "\n",
    "\n",
    "\n",
    "### Overview\n",
    "Using Spark Dataset API  \n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    # initialize Spark Session\n",
    "    import os\n",
    "    import sys\n",
    "    top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "    if top_dir not in sys.path:\n",
    "        sys.path.append(top_dir)\n",
    "\n",
    "    from init_spark import init_spark\n",
    "    spark = init_spark()\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Data\n",
    "Here is  [/data/people/people2.csv](/data/people/people2.csv)\n",
    "```\n",
    "name,gender,age\n",
    "John,M,35\n",
    "Jane,F,40\n",
    "Mike,M,18\n",
    "Sue,F,19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Use CSV Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "peopleDF = spark.read.\\\n",
    "           option(\"header\", \"true\").\\\n",
    "           option(\"inferSchema\", \"true\").\\\n",
    "           csv(\"../data/people/people2.csv\")\n",
    "\n",
    "\n",
    "# for cloud systems\n",
    "# peopleDF = spark.read.\\\n",
    "#            option(\"header\", \"true\").\\\n",
    "#            option(\"inferSchema\", \"true\").\\\n",
    "#            csv(\"s3://elephantscale-public/data/people/people2.csv\")\n",
    "\n",
    "print(peopleDF)\n",
    "peopleDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Queries\n",
    "**==> Find people who are over 30 years old**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## hint: age\n",
    "peopleDF.filter(\"??? > 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Display people by their age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hint : age\n",
    "peopleDF.orderBy(\"???\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# descending order\n",
    "peopleDF.orderBy(\"???\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Specify Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType\n",
    "\n",
    "my_schema = StructType([\n",
    "    \n",
    "    # TODO : gender is 'StringType()' \n",
    "    StructField(\"name\", ???(), True),\n",
    "    \n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    \n",
    "    # TODO : age is 'IntegerType()'\n",
    "    StructField(\"age\", ???(), True),\n",
    "])\n",
    "\n",
    "p = spark.read.\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    schema(my_schema).\\\n",
    "    csv(\"../data/people/people2.csv\")\n",
    "\n",
    "## cloud systems\n",
    "# p = spark.read.\\\n",
    "#     option(\"header\", \"true\").\\\n",
    "#     schema(my_schema).\\\n",
    "#     csv(\"s3://elephantscale-public/data/people/people2.csv\")\n",
    "\n",
    "p.printSchema()\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Step-5: Dealing with Schema Mistmatches\n",
    "\n",
    "Here we will parse [people3.csv](people3.csv) file that looks like this\n",
    "\n",
    "```csv\n",
    "name,gender,age\n",
    "John,M,35\n",
    "Jane,F,40\n",
    "Mike,M,18\n",
    "Sue,F,x\n",
    "```\n",
    "\n",
    "The age for Sue is `x` - not a number!  Let's see how Spark handles this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType, StructField, StructType\n",
    "\n",
    "my_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "# here people3.csv has an invalid value for 'age'\n",
    "# Check out how it is handled.\n",
    "p = spark.read.\\\n",
    "    option(\"header\", \"true\").\\\n",
    "    schema(my_schema).\\\n",
    "    csv(\"people3.csv\")\n",
    "\n",
    "## For cloud systems\n",
    "# p = spark.read.\\\n",
    "#     option(\"header\", \"true\").\\\n",
    "#     schema(my_schema).\\\n",
    "#     csv(\"s3://elephantscale-public/data/people/people3.csv\")\n",
    "\n",
    "p.printSchema()\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Dealing with Null Values\n",
    "\n",
    "Spark has various utilities to deal with null values.  One of them is `na` module.  Here is a way to drop all null values.\n",
    "You will see all rows with null values are dropped.\n",
    "\n",
    "Check out na module documentation [here](https://spark.apache.org/docs/3.2.0/api/python/reference/api/pyspark.sql.DataFrame.na.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = p.na.drop()\n",
    "clean.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
