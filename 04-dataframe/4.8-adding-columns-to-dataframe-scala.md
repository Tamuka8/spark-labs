<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md)

# Lab 4.8: Adding columns to a DataFrame

### Overview
Comparing different data formats for Dataframes.  We will evaluate JSON, Parquet and ORC format.

Background reads:

* https://sparkbyexamples.com/spark/spark-add-new-column-to-dataframe/

### Depends On
None

### Run time
20-30 mins

## Step 1 - create some data

* First, letâ€™s create a simple DataFrame to work with.
```scala
val data = Seq(("111",50000),("222",60000),("333",40000))
val df = data.toDF("EmpId","Salary")
df.show
```

* You will see this output
```text
scala> df.show
+-----+------+
|EmpId|Salary|
+-----+------+
|  111| 50000|
|  222| 60000|
|  333| 40000|
+-----+------+
```

* Or

```scala
df.createOrReplaceTempView("employees")
spark.sql("select EmpId, Salary from employees").show
```
## Step 2 - using withColumn() to add a new column

* withColumn() function takes two arguments, the first argument is the name of the new column and the second argument is the value of the column in `Column` type.

```scala
val df1 = df.withColumn("Monthly", ceil(col("salary") / 12))
df1.show
```
* You will see this output
```text
scala> df1.show
+-----+------+-------+
|EmpId|Salary|Monthly|
+-----+------+-------+
|  111| 50000|   4167|
|  222| 60000|   5000|
|  333| 40000|   3334|
+-----+------+-------+
```

## Step 3 - add multiple columns

```scala
val df2 = df.withColumn("Monthly", ceil(col("salary") / 12)).withColumn("Hourly",ceil(col("salary") / 2000))
df2.show
```

* Or
```scala
spark.sql("select EmpId, Salary, ceil(Salary/12) as Monthly, ceil(Salary/2000) as Hourly from files").show
```
## Step 4 - process a file list

* Create a file list. Verify that you are in the `spark-labs/04-dataframe` directory
 
```shell
ls ../assets/files/ > file-inventory.txt
```

* Read this file list as a DataFrame, create a new DataFrame with additional fields representing metadata, and output the resulting DataFrame.

```scala
val fileName = "file-inventory.txt"
val df = spark.read.textFile(fileName)
df.createOrReplaceTempView("files")
spark.sql("select * from files")
spark.sql("select value as fileName, length(value) as Length from files").show
```

## Bonus

* If you want to extract more information from the file (such as metadata,) 
you will have to use a UDF.

## Bonus answer

* Create a UDF
```scala
val logged = (s: Long) => {
  scala.math.log(s)
}
// register the UDF
spark.udf.register("log", logged)
```

* Go back to employees
```scala
val df = data.toDF("EmpId","Salary")
df.createOrReplaceTempView("employees")
spark.sql("select EmpId, Salary from employees").show
```

* And now, call the UDF

```scala
spark.sql("select EmpId, Salary, log(Salary) as Log from employees").show
```