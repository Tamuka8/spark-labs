{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4.2 : Spark SQL\n",
    "\n",
    "### Overview\n",
    "Using SQL statements with Spark SQL.   \n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    # initialize Spark Session\n",
    "    import os\n",
    "    import sys\n",
    "    top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../\"))\n",
    "    if top_dir not in sys.path:\n",
    "        sys.path.append(top_dir)\n",
    "\n",
    "    from init_spark import init_spark\n",
    "    spark = init_spark()\n",
    "\n",
    "print('Spark UI running on port ' + spark.sparkContext.uiWebUrl.split(':')[2])\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark application UI (4040+)\n",
    "\n",
    "\n",
    "**As you run through the lab keep an eye on Spark application ui on port 4040+.  Inspect how each operation is executed!**\n",
    "\n",
    "![](../assets/images/5.2c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Load Clickstream data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstreamDF = spark.read.json(\"../data/click-stream/clickstream.json\")\n",
    "## FOR CLOUD ACCOUNTS USE THIS\n",
    "# clickstreamDF = spark.read.json(\"s3://elephantscale-public/data/click-stream/clickstream.json\")\n",
    "\n",
    "print(clickstreamDF)\n",
    "clickstreamDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Querying using SQL\n",
    "\n",
    "**==> Register dataframe as a table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clickstreamDF.createOrReplaceTempView(\"clickstream\")\n",
    "print(\"created clickstream temp table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = spark.sql(\"select * from clickstream\")\n",
    "#'logs' is a dataframe\n",
    "\n",
    "# display the logs, hint : show\n",
    "logs.show()\n",
    "\n",
    "# or one liner\n",
    "# spark.sql(\"select * from clickstream\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Find records with only clicks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Find Records with only clicks\n",
    "# TODO: fix the  table name?\n",
    "spark.sql(\"select * from ???table_name??? where action = '???'\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Count records from each domain, sort the output by most to least**\n",
    "\n",
    "Sample output:\n",
    "\n",
    "```\n",
    "+-----------------+-----+\n",
    "|           domain|total|\n",
    "+-----------------+-----+\n",
    "|      nytimes.com|    1|\n",
    "|     facebook.com|    1|\n",
    "|       google.com|    2|\n",
    "|       amazon.com|    2|\n",
    "|    wikipedia.org|    3|\n",
    "+-----------------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Try this query here:\n",
    "\n",
    "spark.sql(\"select domain, COUNT(*) as total from clickstream group by ???  order by ??? \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==>Action: Inspect 4040+ UI.  And see how the execution happens**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## STEP 3: Joining Datasets\n",
    "\n",
    "**==> Load `domains` dataset and register it to table `domains`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "domainsDF = spark.read.json(\"../data/click-stream/domain-info.json\")\n",
    "\n",
    "## For cloud accounts use this\n",
    "# domainsDF = spark.read.json(\"s3://elephantscale-public/data/click-stream/domain-info.json\")\n",
    "\n",
    "domainsDF.show()\n",
    "domainsDF.createOrReplaceTempView(\"domains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**==> Join `clickstreams` and `domains`**    \n",
    "Hint : Join query syntax for joining two tables A, B on A.x and A.y is\n",
    "```\n",
    "spark.sql(\"select A.*, B.* from A  join B  ON (A.x = B.y)\") \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "sql"
     ],
     "id": ""
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Write sql query for joining clickstreams and domains\n",
    "# TODO : fill in the common attribute for  clickstream & domain table\n",
    "# Hint : domain\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT clickstream.*, domains.*  \n",
    "from clickstream join domains \n",
    "ON (clickstream.???= domains.???)\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**==> Count traffic per domain category (news, social ..etc)**    \n",
    "Hint : query the joined datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Count traffic per domained category\n",
    "\n",
    "s = \"\"\"\n",
    "SELECT  domains.category, count(*) as total  \n",
    "from clickstream join domains \n",
    "ON (clickstream.domain= domains.domain)\n",
    "group by ???\n",
    "order by ???\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(s).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## STEP 4: Explore UI\n",
    "(Your DAG visualization may be slightly different from what we show here)\n",
    "\n",
    "<img src=\"../assets/images/5.2c.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n",
    "\n",
    "Look at SQL execution details\n",
    "\n",
    "<img src=\"../assets/images/5.2d.png\" style=\"border: 5px solid grey; max-width:100%;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Understand Query Execution\n",
    "We will use **explain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"select * from clickstream where cost > 100\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"join query here\").explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice the optimizations!\n",
    "\n",
    "There are few optimizations here\n",
    "\n",
    "* broadcast join\n",
    "* filtering on `domain != null`\n",
    "* pushed down predicates  `domain != null`\n",
    "\n",
    "![](../assets/images/dataframe-7-explain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Bonus Lab - Adaptive Query Engine (AQE)\n",
    "\n",
    "AQE is a new shiny SQL engine for Spark 3.  We will run some experiments with AQE.\n",
    "\n",
    "Here is a [sample notebook](4.2b-aqe.ipynb) you can get started with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
