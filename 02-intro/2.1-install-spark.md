<link rel='stylesheet' href='../assets/css/main.css'/>

[<< back to main index](../README.md) 

# Lab 2.1 : Up and Running With Spark

### Overview
We will be running Spark in a single node mode.

### Depends On 
None

### Run time
20 mins

## STEP 0: To Instructor
Please go through this lab on 'screen' first.

## STEP 1: Login to your Spark node
Instructor will provide details


## STEP 2: Installing Spark
There is no 'install'.  Just unzip/untar and run :-)
(copy paste the following commands on terminal,  do not include $ in your commands)

```bash
    $   cd
    $   rm -rf  spark   # cleanup existing spark installation (if any)
    $   tar xvf   files/spark-2.1.0-bin-hadoop2.7.tgz
    $   mv  spark-2.1.0-bin-hadoop2.7    spark
```

Now we have spark installed in  `~/spark`  directory


## STEP 3: Running Spark

```bash
    $   ~/spark/sbin/start-all.sh
```

Verify Spark is running by 'jps' command
```bash
    $  jps
```

Your output may look like this..
```console
  30624 Jps
  30431 Master
  30565 Worker
```
you will see **Master** and **Worker**  processes running.
(you probably will get different values for process ids - first column )

Spark UI will be at port 8080 of the host.
In browser go to
  http://your_spark_host_address:8080
(be sure to use the 'public' ip address)

bingo!  Now we have spark running.


## STEP 4: Exploring Spark UI
You will see a similar screen shot like this

<img src="../assets/images/1a.png" style="border: 5px solid grey ; max-width:100%;" /> 

To explore:
* Is Master and Worker running on the same node?

* Inspect memory & CPU available for Spark worker

* Note the Spark master URI, it will be something like
      spark://host_name:7077
    We will need this for later labs


## STEP 5: Download Spark labs
```bash
    $   cd
    $   git clone   git@github.com:elephantscale/spark-labs.git

    # for v2 - latest
    # $  cd  ~/spark-labs
    # $  git checkout master

    # for 1.6 (old)
    # $  cd ~/spark-labs
    # $  git checkout v1.6
```

This will create a `spark-labs` directory that has all the labs.

## Optional 1: Spark VM

Here are a virtual machine for you

CentOS: https://s3.amazonaws.com/elephantscale-public/vm/CentOS.ova   
Ubuntu: https://s3.amazonaws.com/elephantscale-public/vm/Ubuntu.ova


They are in the OVA format, useable both in VMWare and VirtualBox. 

User name: es   
Password: student

## Optional 2: Running Spark on Windows

If one has to do a Windows install, here is the magic

http://nishutayaltech.blogspot.com/2015/04/how-to-run-apache-spark-on-windows7-in.html

set winutils as described

For example

    \projects
    \projects\spark-labs
    \projects\spark-1.4.1-bin-hadoop2.4
    \projects\winutils\bin\winutils

System env variable

    HADOOP_HOME=\projects\winutils

With this download spark-1.4.1-bin-hadoop2.4
