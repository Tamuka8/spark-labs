{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1 : Up and Running With Spark\n",
    "\n",
    "### Overview\n",
    "We will be running Spark in a single node mode.\n",
    "\n",
    "### Depends On \n",
    "None\n",
    "\n",
    "### Run time\n",
    "20 mins\n",
    "\n",
    "## STEP 0: To Instructor\n",
    "Please go through this lab on 'screen' first.\n",
    "\n",
    "## STEP 1: Login to your Spark node\n",
    "Instructor will provide details\n",
    "\n",
    "\n",
    "## STEP 2: Installing Spark\n",
    "**Skip this step, if you had already installed Spark**\n",
    "\n",
    "There is no 'install'.  Just unzip/untar and run :-)  \n",
    "(copy paste the following commands on terminal,  do not include $ in your commands)\n",
    "\n",
    "```bash\n",
    "$   cd    # go to home directory\n",
    "$   rm -rf  spark   # cleanup existing spark installation (if any)\n",
    "\n",
    "## download\n",
    "$   wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "# alternative download location\n",
    "# $   wget  https://elephantscale-public.s3.amazonaws.com/downloads/spark-3.2.1-bin-hadoop3.2.tgz\n",
    "\n",
    "## unpack\n",
    "$   tar xvf spark-3.2.1-bin-hadoop3.2.tgz\n",
    "$   mv  spark-3.2.1-bin-hadoop3.2    spark\n",
    "```\n",
    "\n",
    "Now we have spark installed in  `~/spark`  directory\n",
    "\n",
    "\n",
    "## STEP 3: Start Spark Master\n",
    "\n",
    "Note: **Spark services are already started** in training docker image.\n",
    "\n",
    "So **you can skip** this step (provided as reference only)\n",
    "\n",
    "**Start master**\n",
    "\n",
    "```bash\n",
    "$   ~/spark/sbin/start-master.sh\n",
    "```\n",
    "\n",
    "Spark master UI will be at port 8080 of the host. \n",
    "\n",
    "In browser go to  http://localhost:8080\n",
    "\n",
    "If you are running on a remote cloud machine,then use the public-ip of your machine\n",
    "  http://your_spark_host_address:8080  \n",
    "\n",
    "\n",
    "Bingo!  Now we have Spark Master running.\n",
    "\n",
    "## Step 4: Inspect Spark Master UI\n",
    "\n",
    "Go ahead and inspect Spark Master UI.\n",
    "\n",
    "Make a note of **Master URL**.  It will be in a format of **`spark://master_host_name:7077`** format.  We will need this when starting the worker\n",
    "\n",
    "## Step 5: Start Spark Worker\n",
    "\n",
    "From terminal execute the following.  Substitute your Spark Master URL\n",
    "\n",
    "```bash\n",
    "## TODO: \n",
    "$   ~/spark/sbin/start-worker.sh   SPARK_MASTER_URL_GOES_HERE\n",
    "```\n",
    "\n",
    "## Step 6: Verify both Master and Worker are running\n",
    "\n",
    "Verify Spark is running by 'jps' command\n",
    "\n",
    "```bash\n",
    "$  jps\n",
    "```\n",
    "\n",
    "Your output may look like this..\n",
    "\n",
    "```  \n",
    "  30624 Jps\n",
    "  30431 Master\n",
    "  30565 Worker\n",
    "```\n",
    "\n",
    "you will see **Master** and **Worker**  processes running.  \n",
    "(you probably will get different values for process ids - first column )\n",
    "\n",
    "### Pro Tip:\n",
    "\n",
    "**Start master & worker with one script**\n",
    "\n",
    "This is a handy script that will start master and workers.  How ever it needs SSH connection setup between hosts.\n",
    "\n",
    "```bash\n",
    "$   ~/spark/sbin/start-all.sh\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## STEP 7: Exploring Spark UI\n",
    "You will see a similar screen shot like this\n",
    "\n",
    "<img src=\"../assets/images/1a.png\" style=\"border: 5px solid grey ; max-width:100%;\" /> \n",
    "\n",
    "To explore:\n",
    "* Is Master and Worker running on the same node?\n",
    "\n",
    "* Inspect memory & CPU available for Spark worker\n",
    "\n",
    "* Note the Spark master URI, it will be something like\n",
    "      spark://host_name:7077\n",
    "    We will need this for later labs\n",
    "\n",
    "\n",
    "## STEP 8: Download Spark labs and Data\n",
    "\n",
    "Please follow setps in [setup-vm-python.ipynb](../setup-vm-python.ipynb)  to download labs and data\n",
    "\n",
    "\n",
    "## Optional: Running Spark on your machine\n",
    "Please follow [setup-local.ipynb](../setup-local.ipynb) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
