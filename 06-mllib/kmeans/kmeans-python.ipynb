{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLLib Example: Clustering\n",
    "\n",
    "Let's look at a clustering example in Spark MLLib.\n",
    "\n",
    "Here, we are going to load the mtcars dataset. This has some stats on different models of cars.  Here, we will load the CSV file as a spark dataframe, and view it.\n",
    "\n",
    "This dataset contains some statistics on 1974 Cars from Motor Trends\n",
    "\n",
    "Here are the columns:\n",
    "* name   - name of the car\n",
    "*  mpg   - Miles/(US) gallon                        \n",
    "*  cyl   - Number of cylinders                      \n",
    "*  disp  - Displacement (cu.in.)                    \n",
    "*  hp    - Gross horsepower                         \n",
    "*  drat  - Rear axle ratio            \n",
    "\n",
    "Are there any natural clusters you can identify from this data?\n",
    "\n",
    "We are going to use **MPG and CYL** attributes to cluster.\n",
    "\n",
    "You can also download and view the raw data in Excel : [cars.csv](/data/cars/mtcars_header.csv)\n",
    "\n",
    "<img src=\"../../assets/images/6.1-cars2.png\" style=\"border: 5px solid grey; max-width:100%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/cars/mtcars_header.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Extract data\n",
    "We only care about 'model', 'mpg' and 'cyl' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : extract the columns we need : model, mpg and cyl\n",
    "dataset2 = dataset.select([\"model\", \"???\", \"???\"])\n",
    "dataset2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Creating Vectors\n",
    "\n",
    "Now that we have ourselves a dataframe, let's work on turning it into vectors.  We're going to vectorize 2 columns:\n",
    "\n",
    "1. MPG\n",
    "2. Number of cylinders.\n",
    "\n",
    "What we'll do, is we'll use the VectorAssembler class to create a new column by the name of features. This will be a Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : create featureVector with 'mpg' and 'cyl'\n",
    "## Hint :  inputCols=['mpg', 'cyl']\n",
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset2)\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Running Kmeans\n",
    "\n",
    "Now it's time to run kmeans on the resultant dataframe. We don't know what value of k to use, so let's just start with k=2.  This means we will cluster into two groups.\n",
    "\n",
    "We will fit a model to the data, and then train it.\n",
    "\n",
    "### 4.1 - Run KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "k = 2\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 -  Predict\n",
    "\n",
    "Notice what we have here.  We have two clusters. One is smaller, fuel efficient cars like the Fiat and the Corolla (remember, we cluster on two variables only: mpg and cyl).  The other is for basically all other cars.  Probably, we can get better results here with a differnet value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(featureVector)\n",
    "predictions.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Calculate the kmeans Silhouette score\n",
    "\n",
    "Silhouette score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means.\n",
    "\n",
    "Silhoutte score ranges from -1 to +1.\n",
    "\n",
    "The silhouette score of 1 means that the clusters are very dense and nicely separated.\n",
    "\n",
    "The score of 0 means that clusters are overlapping\n",
    "\n",
    "The score of less than 0 means that data belonging to clusters may be wrong/incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print (\"k={}, silhouette_score={}\".format(k, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Adjust K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(featureVector)\n",
    "predictions.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print (\"k={}, silhouette_score={}\".format(k, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Iterate over K\n",
    "We are going to calculate WSSSE for various values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kvals = []\n",
    "silhouettes = []\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "for k in range(2,33):\n",
    "    kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "    model = kmeans.fit(featureVector)\n",
    "    predictions = model.transform(featureVector)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print (\"k={}, silhouette_score={}\".format(k, silhouette))\n",
    "    \n",
    "    kvals.append(k)\n",
    "    silhouettes.append(silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Plot Silhouette graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(kvals, silhouettes)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
