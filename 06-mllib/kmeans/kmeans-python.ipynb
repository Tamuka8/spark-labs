{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLLib Example: Clustering\n",
    "\n",
    "### Download the [spreadsheet](WSSSE-versus-k.xlsx)\n",
    "\n",
    "Let's look at a clustering example in Spark MLLib.\n",
    "\n",
    "Here, we are going to load the mtcars dataset. This has some stats on different models of cars.  Here, we will load the CSV file as a spark dataframe, and view it.\n",
    "\n",
    "This dataset contains some statistics on 1974 Cars from Motor Trends\n",
    "\n",
    "Here are the columns:\n",
    "* name   - name of the car\n",
    "*  mpg   - Miles/(US) gallon                        \n",
    "*  cyl   - Number of cylinders                      \n",
    "*  disp  - Displacement (cu.in.)                    \n",
    "*  hp    - Gross horsepower                         \n",
    "*  drat  - Rear axle ratio            \n",
    "\n",
    "Are there any natural clusters you can identify from this data?\n",
    "\n",
    "We are going to use **MPG and CYL** attributes to cluster.\n",
    "\n",
    "You can also download and view the raw data in Excel : [cars.csv](/data/cars/mtcars_header.csv)\n",
    "\n",
    "<img src=\"../../assets/images/6.1-cars2.png\" style=\"border: 5px solid grey; max-width:100%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"/data/cars/mtcars_header.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : print schema\n",
    "## Hint : printSchema()\n",
    "dataset.???()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : display the data\n",
    "## Hint : show\n",
    "dataset.???()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Extract data\n",
    "We only care about 'model', 'mpg' and 'cyl' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : extract the columns we need : model, mpg and cyl\n",
    "dataset2 = dataset.select([\"model\", \"???\", \"???\"])\n",
    "dataset2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Creating Vectors\n",
    "\n",
    "Now that we have ourselves a dataframe, let's work on turning it into vectors.  We're going to vectorize 2 columns:\n",
    "\n",
    "1. MPG\n",
    "2. Number of cylinders.\n",
    "\n",
    "What we'll do, is we'll use the VectorAssembler class to create a new column by the name of features. This will be a Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : create featureVector with 'mpg' and 'cyl'\n",
    "## Hint :  inputCols=['mpg', 'cyl']\n",
    "assembler = VectorAssembler(inputCols=[\"mpg\", \"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset2)\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Running Kmeans\n",
    "\n",
    "Now it's time to run kmeans on the resultant dataframe. We don't know what value of k to use, so let's just start with k=2.  This means we will cluster into two groups.\n",
    "\n",
    "We will fit a model to the data, and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)\n",
    "wssse = model.computeCost(featureVector)\n",
    "\n",
    "print(wssse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WSSSE for this is not particularly good.  We will probably need to change k.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Display grouping\n",
    "Let's take a look at the transformed dataset.  Notice the new column \"prediction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.transform(featureVector)\n",
    "predicted.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what we have here.  We have two clusters. One is smaller, fuel efficient cars like the Fiat and the Corolla (remember, we cluster on two variables only: mpg and cyl).  The other is for basically all other cars.  Probably, we can get better results here with a differnet value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Adjust K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)\n",
    "wssse = model.computeCost(featureVector)\n",
    "\n",
    "print('WSSSE: ' + str(wssse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much better result for WSSSE (lower is better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.transform(featureVector)\n",
    "predicted.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Iterate over K\n",
    "We are going to calculate WSSSE for various values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvals = []\n",
    "wssses = []\n",
    "\n",
    "for k in range(2,33):\n",
    "    kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "    model = kmeans.fit(featureVector)\n",
    "    wssse = model.computeCost(featureVector)\n",
    "    print (\"k\", k , \"wssse\", wssse)\n",
    "    kvals.append(k)\n",
    "    wssses.append(wssse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Plot K vs WSSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(kvals, wssses)\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
