{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLLib Example: Clustering\n",
    "\n",
    "Let's look at a clustering example in Spark MLLib.\n",
    "\n",
    "Here, we are going to load the mtcars dataset. This has some stats on different models of cars.  Here, we will load the CSV file as a spark dataframe, and view it.\n",
    "\n",
    "This dataset contains some statistics on 1974 Cars from Motor Trends\n",
    "\n",
    "Here are the columns:\n",
    "* name   - name of the car\n",
    "*  mpg   - Miles/(US) gallon                        \n",
    "*  cyl   - Number of cylinders                      \n",
    "*  disp  - Displacement (cu.in.)                    \n",
    "*  hp    - Gross horsepower                         \n",
    "*  drat  - Rear axle ratio            \n",
    "\n",
    "Are there any natural clusters you can identify from this data?\n",
    "\n",
    "We are going to use **MPG and CYL** attributes to cluster.\n",
    "\n",
    "You can also download and view the raw data in Excel : [cars.csv](/data/cars/mtcars_header.csv)\n",
    "\n",
    "<img src=\"../../assets/images/6.1-cars2.png\" style=\"border: 5px solid grey; max-width:100%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Spark Session\n",
    "import os\n",
    "import sys\n",
    "top_dir = os.path.abspath(os.path.join(os.getcwd(), \"../../\"))\n",
    "if top_dir not in sys.path:\n",
    "    sys.path.append(top_dir)\n",
    "\n",
    "from init_spark import init_spark\n",
    "spark = init_spark()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.csv(\"../data/cars/mtcars_header.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : print schema\n",
    "## Hint : printSchema()\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : display the data\n",
    "## Hint : show\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Extract data\n",
    "We only care about 'model', 'mpg' and 'cyl' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2 = dataset.select([\"model\", \"mpg\", \"cyl\"])\n",
    "dataset2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Creating Vectors\n",
    "\n",
    "Now that we have ourselves a dataframe, let's work on turning it into vectors.  We're going to vectorize 2 columns:\n",
    "\n",
    "1. MPG\n",
    "2. Number of cylinders.\n",
    "\n",
    "What we'll do, is we'll use the VectorAssembler class to create a new column by the name of features. This will be a Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : create featureVector with 'mpg' and 'cyl'\n",
    "## Hint :  inputCols=['mpg', 'cyl']\n",
    "assembler = VectorAssembler(inputCols=[\"???\", \"???\"], outputCol=\"features\")\n",
    "featureVector = assembler.transform(dataset2)\n",
    "featureVector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WSSSE calcluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from : https://spark.apache.org/docs/3.0.0-preview/mllib-clustering.html\n",
    "\n",
    "# # Build the model (cluster the data)\n",
    "# clusters = KMeans.train(parsedData, 2, maxIterations=10, initializationMode=\"random\")\n",
    "\n",
    "# # Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "# def error(point):\n",
    "#     center = clusters.centers[clusters.predict(point)]\n",
    "#     return sqrt(sum([x**2 for x in (point - center)]))\n",
    "\n",
    "# WSSSE = parsedData.map(lambda point: error(point)).reduce(lambda x, y: x + y)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calculate_wsse (predictions, model):\n",
    "    centers = model.clusterCenters()\n",
    "    \n",
    "    wssse = 0\n",
    "    for i, row in enumerate(predictions.collect()):\n",
    "        feature = row['features']\n",
    "        prediction = row['prediction']\n",
    "        center = centers[prediction]\n",
    "        diff = feature - center\n",
    "        error = np.sqrt(sum([x**2 for x in diff]))\n",
    "        wssse += error\n",
    "        # print ('row={}, feature : {} , prediction : {} , center : {},  diff : {}, error : {},  wssse={}'.\n",
    "        #      format(i, feature, prediction, center, diff, error, wssse))\n",
    "        \n",
    "    return wssse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Running Kmeans\n",
    "\n",
    "Now it's time to run kmeans on the resultant dataframe. We don't know what value of k to use, so let's just start with k=2.  This means we will cluster into two groups.\n",
    "\n",
    "We will fit a model to the data, and then train it.\n",
    "\n",
    "### 4.1 - Run KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "k = 2\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 -  Predict\n",
    "\n",
    "Notice what we have here.  We have two clusters. One is smaller, fuel efficient cars like the Fiat and the Corolla (remember, we cluster on two variables only: mpg and cyl).  The other is for basically all other cars.  Probably, we can get better results here with a differnet value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(featureVector)\n",
    "predictions.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Calculate the kmeans WSSSE and Silhouette score\n",
    "\n",
    "**WSSSE** is an error metric that calcluates how far away points is from their center.  The less the better.\n",
    "\n",
    "**Silhouette** score is used to evaluate the quality of clusters created using clustering algorithms such as K-Means.  \n",
    "Silhoutte score ranges from -1 to +1.  \n",
    "The silhouette score of 1 means that the clusters are very dense and nicely separated.  \n",
    "The score of 0 means that clusters are overlapping.  \n",
    "The score of less than 0 means that data belonging to clusters may be wrong/incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "wssse = calculate_wsse(predictions, model)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print (\"k={}, wssse={},  silhouette_score={}\".format(k, wssse, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Adjust K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "model = kmeans.fit(featureVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(featureVector)\n",
    "predictions.orderBy(['prediction', 'mpg']).show(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "\n",
    "wssse = calculate_wsse(predictions, model)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print (\"k={}, wssse={},  silhouette_score={}\".format(k, wssse, silhouette))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Iterate over K\n",
    "We are going to calculate WSSSE for various values of K:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kvals = []\n",
    "silhouettes = []\n",
    "wssses = []\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "for k in range(2,33):\n",
    "    kmeans = KMeans().setK(k).setMaxIter(10)\n",
    "    model = kmeans.fit(featureVector)\n",
    "    predictions = model.transform(featureVector)\n",
    "    wssse = calculate_wsse (predictions, model)\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print (\"k={}, wssse={},  silhouette_score={}\".format(k, wssse, silhouette))\n",
    "    \n",
    "    kvals.append(k)\n",
    "    wssses.append(wssse)\n",
    "    silhouettes.append(silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Plot Silhouette / WSSSE graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(kvals, silhouettes)\n",
    "pyplot.xlabel('k')\n",
    "pyplot.ylabel('silhouette score')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(kvals, wssses)\n",
    "pyplot.xlabel('k')\n",
    "pyplot.ylabel('wssse')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
