Spark Shell - Python
-----------

-------
== STEP 1) Launch the python shell
-------
  $   cd ~/spark
  $    ./bin/pyspark

This will run the Spark shell application.  Watch the console output.
Once running, you will be presented with >>> prompt.

This is a python shell (you can try entering 2*3 :-)

Spark shell UI is available on port 4040.
In browser go to :   http://your_machine_address:4040
(use 'public' ip of machine)


-------
== STEP 2) Exploring Spark shell UI
-------
In a browser go to to :  http://your_spark_host:4040
Explore stage, storage, environment and executor tabs


-------
== STEP 3) Spark context
-------
Within Spark  shell,  variable 'sc' is the SparkContext
Type 'sc' and see what happens
  > sc

output will be similar to
  <pyspark.context.SparkContext object at 0x7ff227cbbf50>


Q : Print the name of application name
      sc.appName

Q : Find the 'Spark master' for the shell
      sc.master

Quit Spark-shell using 'exit'  or pressing  Control+D


-------
== STEP 4) Load a file
-------
We have data files under 'spark-labs/data'
For ease of use, let's start spark-shell from spark-labs directory

  $    cd  ~/spark-labs
  $    ~/spark/bin/pyspark
you should see the familiar   >>>  prompt

Use test file :  data/twinkle/sample.txt

  scala>
        val f = sc.textFile("data/twinkle/sample.txt")

Q : what is the 'type' of f ?


===========


Q : print the first line / record from RDD
hint : use 'f.first()' function

Q : print first 3 lines of RDD
hint : take
hint : try appending the following at the end of take
      ...().foreach(println)


Q : print all the content from the file
hint : collect()

Q : How many lines are in the file?
hint : count()

Q : Inspect the 'Stages' section in Shell UI (in browser)
Explore what is happening


----------
== STEP 5 )  Connecting Shell and  Spark server
----------

Quit spark-shell session (Control + D)

If Spark server is not running, start it as
  $   ~/spark/sbin/start-all.sh

Inspect the UI at port 8080 to make sure Spark server is running

Now start spark shell
  $    ~/spark/bin/spark-shell

Once the shell starts, check the _server_ UI on port 8080.

Q : Do you see the shell connected as an application?  why (not) ?

Let's connect Spark shell with the Spark server.
Make a note of Spark server uri (e.g  spark://host_name:7077)

Quit spark shell (Control+D)

Restart spark shell as follows
    $   ~/spark/bin/spark-shell   --master  spark-server-uri
                                            ^^^^^^^^^^^^^^^^
                                    update this to match your spark server
e.g.
   $   ~/spark/bin/spark-shell   --master  spark://localhost:7077


Once the shell started, check both UI
  spark server UI at port 8080
  spark shell UI at  port 4040

Explore both UIs

Redo step (4)  in this new environment

========

