[<< back to main index](../README.md)

Lab 4.1 First Spark Job Submission
==================================

### Overview
Submit a job for Spark

### Depends On 
None

### Run time
20-30 mins


### STEP 1: Edit source file

Go to the project root directory

    $    cd ~/spark-labs/4-api


**=> Edit file : `src/main/scala/x/ProcessFiles.scala`**  
**=> And fix the TODO items**

    $    vi  src/main/scala/x/ProcessFiles.scala
    # or 
    $    nano  src/main/scala/x/ProcessFiles.scala


### STEP 2: Compile the project

We will use `sbt` to build the project.  

**=> Inspect the `build.sbt` file**

    $  cat   build.sbt

The file will look follows:

    // blank lines are important!
    
    name := "TestApp"
    
    version := "1.0"
    
    scalaVersion := "2.10.4"
    
    libraryDependencies ++= Seq(
      "org.apache.spark" %% "spark-core" % "1.4.1" % "provided"
    )
    
    // for accessing files from S3 or HDFS
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.4.0" exclude("com.google.guava", "guava")


**=> Kick off a build**  
(This will take a few minutes for the first time you run it)

    $   sbt package
    # to do a clean rebuild use
    #  sbt clean package

Make sure there are no errors and there is output in `target` dir.

    $  ls -l   target/scala-2.10

You should see output like follows

    drwxr-xr-x  3 sujee  staff   102B Apr 16 09:59 classes/
    -rw-r--r--  1 sujee  staff    13K Apr 16 09:59 testapp_2.10-1.0.jar

`testapp_2.10-1.0.jar `  is our code compiled.
 

### STEP 3: Start Spark Server

    $  ~/spark/sbin/start-all.sh


**=> Check the Spark Server UI at port 8080.**  
**=> Note the Spark master URL.**  

<img src="../images/5.1b.png" style="border: 5px solid grey; max-width:100%;"/>


### STEP 4: Submit the application

Use the following command to submit the job

    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master MASTER_URL  target/scala-2.10/testapp_2.10-1.0.jar    <files to process>

* MASTER URL : substitute your spark master url
* for files you can try `README.md`

Here is an example

  $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master spark://localhost:7077  target/scala-2.10/testapp_2.10-1.0.jar    README.md


**=> Watch the console output**

It may look like this

    15/04/15 15:41:51 INFO SparkUI: Started SparkUI at http://192.168.1.115:4040
    ...
    15/04/15 15:41:54 INFO DAGScheduler: Job 0 finished: count at ProcessFiles.scala:42, took 2.156893 s

    ### README.md : count (7) took 2,251.007000 ms


The lines starting with `###` are output from our program


### STEP 5:  Turn off logging

Spark by default logs at INFO level.  While there is a lot of useful information there, it can be quite verbose.

We have a `logging/log4j.properties` file.  Inspect this file

    $    cat   logging/log4j.properties


The file has following contents

    # Set everything to be logged to the console
    log4j.rootCategory=WARN, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.err
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n
    
    # Settings to quiet third party logs that are too verbose
    log4j.logger.org.eclipse.jetty=WARN
    log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
    log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO



We provide `--driver-class-path logging/`  to spark-submit to turn off logging

Here is an example

    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master spark://localhost:7077  --driver-class-path logging/  target/scala-2.10/testapp_2.10-1.0.jar    README.md


### STEP 6:  Read a file from Amazon S3

For the file argument, try the following `s3n://elephantscale-public/data/twinkle/100M.data`

For example:

    $   ~/spark/bin/spark-submit --class 'x.ProcessFiles' --master spark://localhost:7077  --drive-class-path logging/  target/scala-2.10/testapp_2.10-1.0.jar    's3n://elephantscale-public/data/twinkle/100M.data'


**=> Notice the time it too to access the file**

**=> To find out more data files on S3 use the following command**

    $   s3cmd ls s3://elephantscale-public/data/twinkle/
