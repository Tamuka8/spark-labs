[<< back to main index](../README.md)

Lab 3.4 : RDD Caching
=====================

### Overview
Understanding RDD caching

### Depends On 
None

### Run time
15-20 mins


---------------------
STEP 1: Generate data
---------------------
If you haven't done so, generate some large data files
```bash
 $    cd ~/spark-labs/data/twinkle
 $    ./create-data-files.sh
```

This script will generate a bunch of data files at various sizes (1M, 10M, 100M, 500M and 1G)
Verify the data files and their sizes by doing a
```bash
  $   ls -lh
```
Your output might look like this  
<img src="../images/3.1a.png" style="border: 5px solid grey; max-width:100%;"/>


----------------------------------
STEP 2: Start Spark Server / Shell
----------------------------------
```
$   cd ~/spark-labs
$  ~/spark/sbin/start-all.sh
```

Starting Shell (with 4G memory)

#### Scala
```
$  ~/spark/bin/spark-shell  --master <spark master uri>  --executor-memory 4G
```

#### Python:
```
$   ~/spark/bin/pyspark   --master  spark-server-uri
#                                   ^^^^^^^^^^^^^^^^
#                                    update this to match your spark server

$   ~/spark/bin/pyspark   --master  spark://localhost:7077  --executor-memory 4G
```



----------------
STEP 3: Load RDD
----------------
Load a big file (e.g 500M.data)

```scala
// scala
val f = sc.textFile("data/twinkle/500M.data")
```

```python
# python
f = sc.textFile("data/twinkle/500M.data")
```

**=> Count the number of lines in this file**  
Hint : `count()`  

**=> Notice the time took**  
**=> Do the same count() operation a few times until the execution time 'stablizes'**  
**=> Can you explain the behavior of count() execution time ?**


--------------
STEP 4:  Cache
--------------
**=> Cache the file using  `cache()` action.**
```
f.cache()
```

**=> Run the `count()` again. Notice the time.   Can you explain this behavior ?  :-)** 

**=> Run count() a few more times and note the execution times.**  
**=> Do the timings make sense?** 


------------------------------------
STEP 5:  Understanding Cache storage
------------------------------------
Go to spark shell UI @ port 4040  
**=> Inspect 'storage' tab**  

<img src="../images/3.4.png" style="border: 5px solid grey; max-width:100%;"/>

**=> Can you see the cached RDD?  What is the memory size?**  
**=> What are the implications?** 

----------------------------
STEP 5:  Cache a larger file
----------------------------
**=> Try to cache 1G.data file and do count()**  
Is caching successful ?
If not, try starting Spark shell with more memory


----------------------------------
Step 6 : Reducing memory footprint 
----------------------------------
There are various levels of memory caching.  Here are a couple:
* Raw caching (`rdd.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY)`)
* Serialized Caching (`rdd.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY_SER)`)

**=> Try both options `f.persist(....)` .  Monitor memory consumption in storage tab**


-----------------
Group discussion
-----------------
* mechanics of caching
* implications of caching vs memory


-------------------------------------
BONUS LAB: Caching data from Amazon S3
-------------------------------------
We have some data files stored in Amazon S3.  Here are couple of path names
* s3n://elephantscale-public/data/twinkle/100M.data
* s3n://elephantscale-public/data/twinkle/500M.data
* and more...

**=> Try loading these files.  Measure performance time before and after caching.**  

```scala
// scala
val f = sc.textFile("s3n://elephantscale-public/data/twinkle/500M.data")
f.count() // measure time.. do it a couple of times
f.cache() 
f.count() // measure time.. do it a couple of times
```

```python
# python
f = sc.textFile("s3n://elephantscale-public/data/twinkle/500M.data")
f.count() // measure time.. do it a couple of times
f.cache() 
f.count() // measure time.. do it a couple of times
```

---------------
Further Reading
---------------
* [Understanding Spark Caching by Sujee Maniyam](http://sujee.net/2015/01/22/understanding-spark-caching/)