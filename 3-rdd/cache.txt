Caching RDDs
============
In this lab, we will cache RDDs in memory for very fast computes.


-----
== STEP 1) Generate data
-----
If you haven't done so, generate some large data files
  $    cd ~/spark-labs/data/twinkle
  $    ./create-data-files.sh
inspect the files created by
  $   ls -lh

-----
== STEP 2) Start Spark Server / Shell
-----
cd to spark-labs dir
  $   cd ~/spark-labs

Starting server:
  $  ~/spark/sbin/start-all.sh

Starting Shell (with 4G memory)

scala:
  $  ~/spark/bin/spark-shell  --master <spark master uri>  --executor-memory 4G

python:
  $   TODO


-----
== STEP 3) load RDD
-----
Load a big file (e.g 500M.data)

Scala
    >  val f = sc.textFile("data/twinkle/500M.data")

Python
    > TODO

count the number of lines in this file
Hint : count()
Notice the time took.
Do the same count() operation a few times until the execution time 'stablizes'

Q : Can you explain the behavior of count() execution time ?


-------
== STEP 4)  Cache
-----
cache the file using  'cache()' action.

Scala:
  >    f.cache()

Python:
  >    TODO

run the count() again.
Notice the time.   Can you explain this behavior ?  :-)

Run count() a few more times and note the execution times.
Do the timings make sense?


----
== STEP 5)  Understanding Cache storage
----
Go to spark shell UI @ port 4040
Inspect 'storage' tab.
Can you see the cached RDD?  What is the size?
What are the implications?


----
== STEP 5)  Cache a larger file
----
Try to cache 1G.data file and do count()
Is caching successful ?
If not, try starting Spark shell with more memory


Group discussion :
  - mechanics of caching
  - implications of caching vs memory


== BONUS LAB ) Look at Spark optimizing guide
TODO :  link
Any hints to do effective caching?
Try it out