<link rel='stylesheet' href='../assets/main.css'/>

[<< back to main index](../README.md) 

Lab 5.3 : Data formats (JSON vs.  Parquet)
==========================================

### Overview
Comparing different data formats for Dataframes.  We will evaluate JSON and Parquet format.

Background reads:
- [Spark data frames](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- JSON format 
    - [wikipedia](https://en.wikipedia.org/wiki/JSON)
    - [json.org](http://json.org/)
- Parquet format
    - [Parquet project](https://parquet.apache.org/)
    - [parquet github](https://github.com/Parquet/parquet-format)
    - [presentation](http://www.slideshare.net/larsgeorge/parquet-data-io-philadelphia-2013)

### Depends On 
None

### Run time
20-30 mins


### STEP 1: Generate Clickstream data

    $    cd   ~/spark-labs/5-sql
    $    python   gen-clickstream-json.py
    # This will generate about 1G logs in `logs-json` dir


**=> Inspect generated json logs**  

    $    head logs-json/2015-01-01.json


### STEP 2: Start Spark Shell & ATOP

(Be sure to be in `~/spark-labs/5-sql` dir)


    # we are giving more memory
    $    ~/spark/bin/spark-shell --executor-memory 2g  --driver-memory 1g


Also open another terminal and run `atop`.  
We will use this to monitor CPU / IO usage 

### STEP 3: Setup SQL Imports

**This is not necessary for Spark 1.3 + **


    // sqlContext is the 'entry point'  into Spark SQL
    val sqlContext = new org.apache.spark.sql.SQLContext(sc)
    
    // this is used to implicitly convert an RDD to a DataFrame.
    import sqlContext.implicits._

### STEP 4: Load Clickstream data
    
    // load all the files in the dir
    val clicksJson = sqlContext.read.json("logs-json/")

**==> While the import is running take a look at `atop` terminal.  Which of the resources are we maxing out?**

**==> Find the max value of cost**   
**==> While the query is running, check `atop`**  

    val maxCost = clicksJson.agg(max("cost"))
    maxCost.show

Sample output

    +---------+
    |MAX(cost)|
    +---------+
    |      180|
    +---------+

**==> Note the time it took to run the query**

    Job 1 finished: show at <console>:24, took `8.550481 s`


### STEP 5 : Save the logs in Parquet format

We are going to use Spark's built-in parquet support to save the dataframe into parquet format


    // spark shell
    clicksJson.write.parquet("logs-parquet")

**==> Inspect `atop`**  
**==> Once the job is completed, inspect the `logs-parquet` directory** 

    $   ls -la   logs-parquet


Output might look like this

    -rw-r--r--  1 sujee  staff     0B Jul 19 12:40 _SUCCESS
    -rw-r--r--  1 sujee  staff   756B Jul 19 12:40 _common_metadata
    -rw-r--r--  1 sujee  staff    56K Jul 19 12:40 _metadata
    -rw-r--r--  1 sujee  staff   1.8M Jul 19 12:39 part-r-00000-9ceb13fe-a57c-4af1-993e-d998d6c41008.gz.parquet
    -rw-r--r--  1 sujee  staff   1.8M Jul 19 12:39 part-r-00001-9ceb13fe-a57c-4af1-993e-d998d6c41008.gz.parquet

**==> Inspect parquet metadata**  

    $    cat logs-parquet/_common_metadata
    $    cat logs-parquet/_metadata


### STEP 6 : Querying Parquet Data
  
    val clicksParquet = sqlContext.read.parquet("logs-parquet")

**==> Note how quickly the schema is inferred!**  
Parquet format has built-in schema, so Spark doesn't have to parse the files as needed in JSON format

**==> Caclculate max(cost)**   

    clicksParquet.agg(max("cost")).show  // same as before

**==> Notice the time took!**    
Sample output

    Job 3 finished: show at <console>:24, took `0.627185 s`


**==> Why parquet is so quick to process?** 

**==> Also compare the data sizes of both formats**  


    $    du -skh  logs-json   logs-parquet


Sample output

    1.3G    logs-json
     77M    logs-parquet


### STEP 7 : Compressed JSON

We are going to store JSON files in compressed gzip format

**==> Compress the files**  

    $    cd   ~/spark-labs/5-sql
    $   ./compress-json.sh

This will create compressed JSON in `logs-json-gz` directory

**==> Inspect directory sizes**  
    
    $    du -skh logs*

Sample output

    1.3G    logs-json
    154M    logs-json-gz
     77M    logs-parquet


**==> Load compressed json files in Spark shell and do the same processing**  
**==> Look at `atop` window to see resource usage**  

    // note the parsing time
    val clicksJgz = sqlContext.read.json("logs-json-gz")

    // calculate the max cost
    // notice the time took
    clicksJgz.agg(max("cost")).show

    // output : Job 7 finished: show at console:22, took 8.066727 s


### STEP 8 : Analyze / discuss results


    Here are numbers from my run:
    
    |format   | storage size |  parsing time | processing time : max(cost)|
    |---------|:-------------|:--------------|:--------------------------:|
    | json    |  1.3 G       |  8.3 s        |   6.9 s                    |
    | json.gz |  154 M       |  8.5 s        |   6.6 s                    | 
    | parquet |   77 M       |    0 s        |   0.6 s                    | 


**==> Also discuss your findings from `atop`.  Which resource 'ceiling' we are hitting first?  CPU / Memory / Disk ?**

