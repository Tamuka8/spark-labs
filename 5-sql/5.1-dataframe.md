[<< back to main index](../README.md) 

Lab 5.1 : Spark SQL : Dataframes
================================

### Overview
First look at Spark SQL

### Depends On 
None

### Run time
20-30 mins


----------------------------
STEP 1: Start Spark Shell
----------------------------
Change working directory to `spark-labs`.  This way, we can access data using relative paths (makes life simpler)
```
  $  cd ~/spark-labs
```

#### == Scala:
```
  $   ~/spark/bin/spark-shell
```
#### == Python
```
  $    ~/spark/bin/pyspark
```


----------------------------
STEP 2: Setup SQL Imports
----------------------------
**This is not necessary for Spark 1.3 + **

```scala
// scala

// sqlContext is the 'entry point'  into Spark SQL
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

// this is used to implicitly convert an RDD to a DataFrame.
import sqlContext.implicits._

```

----------------------------
STEP 3: Load Clickstream data
----------------------------
```scala
// scala

val clickstreamDF = sqlContext.read.json("data/click-stream/clickstream.json")

// 1.3
// val clickstreamDF = sqlContext.jsonFile("data/click-stream/clickstream.json")
```

**==> Spark will process the file to infer the schema ;  See console output**  

```output
...
clickstreamDF: org.apache.spark.sql.DataFrame = [action: string, campaign: string, 
cost: bigint, domain: string, ip: string, session: string, 
timestamp: bigint, user: string]
```

**==> Monitor Spark shell UI on port 4040**  
You may see something like this:

<img src="../images/6.1a.png" style="border: 5px solid grey; max-width:100%;" />


---------------------------------
STEP 4 : Inspecting The Dataframe
---------------------------------
**==> Print the schema of data frame**   
Hint : `clickstreamDF.printSchema`
Your output will look like this:

```
root
 |-- action: string (nullable = true)
 |-- campaign: string (nullable = true)
 |-- cost: long (nullable = true)
 |-- domain: string (nullable = true)
 |-- ip: string (nullable = true)
 |-- session: string (nullable = true)
 |-- timestamp: long (nullable = true)
 |-- user: string (nullable = true)
```


**==> Print / Dump the data contained within dataframe**  
Hint : `clickstreamDF.show`

Your output may look like this:
```
+-------+-----------+----+-----------------+----+----------+-------------+------+
| action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
+-------+-----------+----+-----------------+----+----------+-------------+------+
|clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
|blocked|campaign_12|   5|     facebook.com|ip_3|session_96|1420070400864|user_5|
|clicked| campaign_3|  54|sf.craigslist.org|ip_9|session_61|1420070401728|user_8|
...
```

**==> Explore methods available in Dataframe**  
1) Here is the Dataframe API : 
[Scala](http://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.DataFrame)  /
[Java](http://spark.apache.org/docs/1.4.0/api/java/org/apache/spark/sql/DataFrame.html) / 
[Python](http://spark.apache.org/docs/1.4.0/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame)

2) Type `clickstreamDF.TAB TAB` (two tabs)  to see all methods available on DataFrame.  

```
scala> clickstreamDF.
agg                 apply               as                  asInstanceOf        cache
col                 collect             collectAsList       columns             count
createJDBCTable     describe            distinct            dtypes              except
explain             explode             filter              first               flatMap
foreach             foreachPartition    groupBy             head                insertInto
insertIntoJDBC      intersect           isInstanceOf        isLocal             javaRDD
join                limit               map                 mapPartitions       na
orderBy             persist             printSchema         queryExecution      rdd
registerTempTable   repartition         sample              save                saveAsParquetFile
saveAsTable         schema              select              selectExpr          show
sort                sqlContext          take                toDF                toJSON
toJavaRDD           toSchemaRDD         toString            unionAll            unpersist
where               withColumn          withColumnRenamed
```

---------------------------------
STEP 5 : Querying Dataframe
---------------------------------

**==> Show only click logs where the cost > 100**  
Hint : `clickstreamDF.filter(clickstreamDF("cost") > 100).show()`

Sample output
```
+-------+-----------+----+-----------------+----+----------+-------------+------+
| action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|
+-------+-----------+----+-----------------+----+----------+-------------+------+
|clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|
|blocked|campaign_18| 110|    wikipedia.org|ip_5|session_55|1420070402592|user_6|
|blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|
```

**==> Show the logs where action = clicked**  
Hint : `clickstreamDF.filter(clickstreamDF("action") == "clicked")`  won't work ! :-)  
Let's figure this out.  What is the type of `clickstreamDF("action")`
```scala
//scala

val c = clickstreamDF("action")
// c: org.apache.spark.sql.Column = action

// to see methods on c use double-TAB
c.[TAB][TAB]
```

You will see methods avilable on on `org.apache.spark.sql.Column`
```
scala> c.
!==            %              &&             *              +              -
/              ===            >              >=             and            as
asInstanceOf   asc            cast           contains       desc           divide
endsWith       eqNullSafe     equalTo        explain        geq            getField
getItem        gt             in             isInstanceOf   isNotNull      isNull
leq            like           lt             minus          mod            multiply
notEqual       or             plus           rlike          startsWith     substr
toString       unary_!        unary_-        ||
```

Now use the correct method to filter 'action = clicked'  
Hint: `===`  or `equalTo`
```
clickstreamDF.filter(clickstreamDF("action") === ??? ).show
or 
clickstreamDF.filter(clickstreamDF("action").equalTo(???).show

```

**==> Count the number of visits from each domain**  
Hint : `val g = clickstreamDF.groupBy("column_name")`  
inspect methods on `g`  (use tab completion)  
then do `count` on `g`  
then do a show  
So `clickstreamDF.groupby().count.show`  


---------------------------
STEP 6 : Joining Dataframes
---------------------------
Let's load another data set `domain info`  
The data is in   `data/click-stream/domain-info.json`  
The data looks like this:
```
{"domain":"amazon.com","category":"SHOPPING"}
{"domain":"bbc.co.uk","category":"NEWS"}
{"domain":"facebook.com","category":"SOCIAL"}
...
```

**==> Load the dataframe**
```scala
val domainsDF = sqlContext.read.json("data/click-stream/domain-info.json")
```

**==> Join both dataframes**
```scala
val joined = clickstreamDF.join(domainsDF,  clickstreamDF("domain") === domainsDF("domain"))

// see the results
joined.show
```

**==> Inspect the results, here is a sample**
```
+-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
| action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
+-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
|clicked|campaign_19| 118|      youtube.com|ip_4|session_36|1420070400000|user_9|      VIDEO|      youtube.com|
|blocked| campaign_2|   7|      youtube.com|ip_2|session_93|1420070412960|user_1|      VIDEO|      youtube.com|
|blocked|campaign_17|  20|       amazon.com|ip_4|session_13|1420070406048|user_1|   SHOPPING|       amazon.com|
```


**==> Note some rows are missing.  Which ones?  Why?**

**==> Do an outer join**  
Hint : provide a third argument `outer` to the join statement  
e.g  `val joinedOuter = clickstreamDF.join(domainsDF,  ......,    "outer")`

**==> Inspect the output, might look like this**  
**==> Can you explain the null values?**  
```
// joinedOuter.show

// result ==> 

+-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
| action|   campaign|cost|           domain|  ip|   session|    timestamp|  user|   category|           domain|
+-------+-----------+----+-----------------+----+----------+-------------+------+-----------+-----------------+
|blocked| campaign_9| 139|          cnn.com|ip_8|session_13|1420070404320|user_7|       null|             null|
|   null|       null|null|             null|null|      null|         null|  null|     SOCIAL|      twitter.com|
|clicked| campaign_6|  15|comedycentral.com|ip_9|session_49|1420070403456|user_4|       null|             null|
```


-----------------
STEP 7: BONUS Lab
-----------------

