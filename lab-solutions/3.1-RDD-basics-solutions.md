3.1 RDD Basics solutions
========================

## Step 1: Start Spark Shell
### == scala 
```bash
$   cd ~/spark-labs
$   ~/spark/bin/spark-shell
```

The following is in scala shell
```scala
val f = sc.textFile("data/twinkle/sample.txt")
val filtered = f.filter(line => line.contains("twinkle"))  // no-op
filtered.count()  // now spark will process it
filtered.collect() // see array as output
// Array[String] = Array(twinkle twinkle little star, twinkle twinkle little star)
exit // quit shell
```

### == python
```bash
$   cd ~/spark-labs
$   bin/pyspark 
```

The following is in python shell
```python
f = sc.textFile("data/twinkle/sample.txt")
filtered = f.filter(lambda line: "twinkle" in line)
filtered.count()
# 2
```

## Step 2: Prepare Data
```bash
$   cd  ~/spark-labs/data/twinkle
$   ./create-data-files.sh
```

## Step 3 : Start Spark shell and connect to Spark Server

### == Scala
```bash
$     cd  ~/spark-labs
# TODO : use the correct spark url for --master
$   ~/spark/bin/spark-shell  --executor-memory 1G --master  spark://localhost:7077
```

### == Python
```bash
$     cd  ~/spark-labs
# TODO : use the correct spark url for --master
$   ~/spark/bin/pyspark  --executor-memory 1G --master  spark://localhost:7077
```

## Step 4 : Process a large file
### == scala
```scala
val f = sc.textFile("data/twinkle/100M.data")

// count lines containing the word 'diamond'
f.filter(line => line.contains("diamond")).count()
// res0: Long = 782519

// count lines does NOT contain 'diamond'
f.filter(line => !line.contains("diamond")).count()
// res1: Long = 2347559

// all count
f.count()
// res2: Long = 3130078

// double check count
782519 + 2347559
// res3: Int = 3130078
// res3 == res2 ... universe is safe ! :-)

// try with larger files
val f2 = sc.textFile("data/twinkle/1G.data")
// and repeat the above steps
```

### == Python
```python
f = sc.textFile("data/twinkle/100M.data")

# count lines containing the word 'diamond'
f.filter(lambda line: "diamond" in line).count()
# 782519

# count lines does NOT contain 'diamond'
f.filter(lambda line: "diamond" not in line).count()
# 2347559

# all count
f.count()
# 3130078

# double check count
782519 + 2347559
# 3130078
# count matches... universe is safe ! :-)

# try with larger files
f2 = sc.textFile("data/twinkle/1G.data")
# and repeat the above steps
```

## Step 5 : Loading multiple files
### == scala:
Following is typed in Scala shell
```scala
val rdd = sc.textFile("data/twinkle/*.data")  
rdd.count()
// res5: Long = 117534419

// filter for 'diamond'
val rddf = rdd.filter(line => line.contains("diamond"))

// save
rddf.saveAsTextFile("out1")
exit
```


### == Python
Following is typed in python shell
```python
rdd = sc.textFile("data/twinkle/*.data")
rddf = rdd.filter(lambda line: "diamond" in line)
rddf.saveAsTextFile("out1")
```

## Inspecting the output dir:
```bash
$  ls -lh  out1

# we will see 121 partitions 00000 --> 00120

-rw-r--r--  1 sujee  staff     0B Apr  1 00:46 _SUCCESS
-rw-r--r--  1 sujee  staff   6.2M Apr  1 00:46 part-00000
-rw-r--r--  1 sujee  staff   6.2M Apr  1 00:46 part-00001
-rw-r--r--  1 sujee  staff   6.2M Apr  1 00:46 part-00002
...
-rw-r--r--  1 sujee  staff   6.2M Apr  1 00:46 part-00119
-rw-r--r--  1 sujee  staff   6.2M Apr  1 00:46 part-00120
```

Inspect a file
```bash
$   less  out1/part-00000

like a diamond in the sky
like a diamond in the sky
like a diamond in the sky
....
```
