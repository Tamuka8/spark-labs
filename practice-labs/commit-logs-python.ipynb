{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Lab 1 - Analyze Spark Commits\n",
    "\n",
    "\n",
    "### Overview\n",
    "Process CSV using Spark2 APIs\n",
    "\n",
    "### Depends On\n",
    "- [4.1 Dataframe](../04-dataframe/4.1-dataframe-scala.md)\n",
    "- [4.2 SQL](../04-dataframe/4.2-sql-scala.md)\n",
    "- [4.3 Dataset](../04-dataframe/4.3-dataset-scala.md)\n",
    "\n",
    "### Run time\n",
    "20-30 mins\n",
    "\n",
    "## Analyze Spark commit logs (CSV data)\n",
    "In this lab, we are going to analyze Spark commit logs.\n",
    "\n",
    "The data is located in `data/spark-commits` directory.\n",
    "- [data/spark-commits/spark-commits.log](/data/spark-commits/spark-commits.log)  : large file with about 20,000 commit logs\n",
    "- [data/spark-commits/sample.log](/data/spark-commits/sample.log) : a small sample file\n",
    "\n",
    "### Data Format:\n",
    "\n",
    "- SHA\n",
    "- committer\n",
    "- email\n",
    "- date\n",
    "- comment\n",
    "\n",
    "Separator is  |\n",
    "```\n",
    "sha|committer|email|date|comment\n",
    "\n",
    "8f2142cfd2ca632a4afb0cc29cc358edbb21f8d|Dilip Biswal|dbiswal@us.ibm.com|Sat Feb 25 23:56:57 2017 -0800|[SQL] Duplicate test exception in SQLQueryTestSuite due to meta files(.DS_Store) on Mac\n",
    "\n",
    "89608cf26226e28f331a4695fee89394d0d38ea0|Wenchen Fan|wenchen@databricks.com|Sat Feb 25 23:01:44 2017 -0800|[SPARK-17075][SQL][FOLLOWUP] fix some minor issues and clean up the code\n",
    "\n",
    "6ab60542e8e803b1d91371a92f4aaef6a64106f6|Joseph K. Bradley|joseph@databricks.com|Sat Feb 25 22:24:08 2017 -0800|[MINOR][ML][DOC] Document default value for GeneralizedLinearRegression.linkPower\n",
    "```\n",
    "\n",
    "## Queries\n",
    "- Find the person with most number of commits\n",
    "- How many commits came from 'databricks.com'\n",
    "\n",
    "## Steps\n",
    "This is an open lab.  \n",
    "Feel free to experiment.  \n",
    "Use any of the APIs (RDD / Dataframe / Dataset) to analyze the data.\n",
    "\n",
    "### Some Hints:\n",
    "\n",
    "`spark.read.csv`  is a handy way to read CSV files.  (see below)\n",
    "\n",
    "For group by, you can use `email` column as a unique column.  \n",
    "\n",
    "You can group by using Dataset API  or use SQL (register DF as a temptable first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commits = spark.read.\\\n",
    "              option(\"header\", \"???\").\\  # True or False?\n",
    "              option(\"delimiter\", \"???\").\\  # what is our delimiter\n",
    "              csv(\"/data/spark-commits/spark-commits.log\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
