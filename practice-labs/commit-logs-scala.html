<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<p><a href="../README.md">&lt;&lt; back to main index</a></p>
<h1 id="practice-lab-1---analyze-spark-commits">Practice Lab 1 - Analyze Spark Commits</h1>
<h3 id="overview">Overview</h3>
<p>Process CSV using Spark2 APIs</p>
<h3 id="depends-on">Depends On</h3>
<ul>
<li><a href="../04-dataframe/4.1-dataframe-scala.md">4.1 Dataframe</a></li>
<li><a href="../04-dataframe/4.2-sql-scala.md">4.2 SQL</a></li>
<li><a href="../04-dataframe/4.3-dataset-scala.md">4.3 Dataset</a></li>
</ul>
<h3 id="run-time">Run time</h3>
<p>20-30 mins</p>
<h2 id="analyze-spark-commit-logs-csv-data">Analyze Spark commit logs (CSV data)</h2>
<p>In this lab, we are going to analyze Spark commit logs.</p>
<p>The data is located in <code>data/spark-commits</code> directory. - <a href="/data/spark-commits/spark-commits.log">data/spark-commits/spark-commits.log</a> : large file with about 20,000 commit logs - <a href="/data/spark-commits/sample.log">data/spark-commits/sample.log</a> : a small sample file</p>
<h3 id="data-format">Data Format:</h3>
<ul>
<li>SHA</li>
<li>committer</li>
<li>email</li>
<li>date</li>
<li>comment</li>
</ul>
<p>Separator is |</p>
<pre><code>sha|committer|email|date|comment

8f2142cfd2ca632a4afb0cc29cc358edbb21f8d|Dilip Biswal|dbiswal@us.ibm.com|Sat Feb 25 23:56:57 2017 -0800|[SQL] Duplicate test exception in SQLQueryTestSuite due to meta files(.DS_Store) on Mac

89608cf26226e28f331a4695fee89394d0d38ea0|Wenchen Fan|wenchen@databricks.com|Sat Feb 25 23:01:44 2017 -0800|[SPARK-17075][SQL][FOLLOWUP] fix some minor issues and clean up the code

6ab60542e8e803b1d91371a92f4aaef6a64106f6|Joseph K. Bradley|joseph@databricks.com|Sat Feb 25 22:24:08 2017 -0800|[MINOR][ML][DOC] Document default value for GeneralizedLinearRegression.linkPower</code></pre>
<h2 id="queries">Queries</h2>
<ul>
<li>Find the person with most number of commits</li>
<li>How many commits came from ‘databricks.com’</li>
</ul>
<h2 id="steps">Steps</h2>
<p>This is an open lab.<br />
Feel free to experiment.<br />
Use any of the APIs (RDD / Dataframe / Dataset) to analyze the data.</p>
<h3 id="some-hints">Some Hints:</h3>
<p><code>spark.read.csv</code> is a handy way to read CSV files.</p>
<pre><code>val commits = spark.read.
              option(&quot;header&quot;, &quot;???&quot;).  // true or false?
              option(&quot;delimiter&quot;, &quot;???&quot;).  // what is our delimiter
              csv(&quot;/data/spark-commits/spark-commits.log&quot;)</code></pre>
<p>For group by, you can use <code>email</code> column as a unique column.</p>
<p>You can group by using Dataset API or use SQL (register DF as a temptable first)</p>
