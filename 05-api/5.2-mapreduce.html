<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<p><a href="../README.md">&lt;&lt; back to main index</a></p>
<h1 id="lab-5.2-mapreduce-application">Lab 5.2 MapReduce Application</h1>
<h3 id="overview">Overview</h3>
<p>Write and submit a MapReduce application</p>
<h3 id="depends-on">Depends On</h3>
<p><a href="5.1-submit.md" class="uri">5.1-submit.md</a></p>
<h3 id="run-time">Run time</h3>
<p>20-30 mins</p>
<h3 id="setup">Setup</h3>
<p>During ‘mapreduce’ lab we did clickstream analysis. In this lab, we are going to write a full fledged MapReduce program and submit it to Spark.</p>
<h2 id="goal-find-click-view-ratio-for-each-domain">Goal Find click-view ratio for each domain</h2>
<p>We want to generate output like this. The output will be sorted by highest view/click ratio</p>
<pre><code># domain, number of total clicks,  number of total views,  views/cliks ratio

npr.org, 10 , 90,  9
facebook.com,  5, 30, 6</code></pre>
<h3 id="step-1-inspect-data">STEP 1: Inspect Data</h3>
<p>Clickstream data has the following format</p>
<pre><code>timestmap, ip_address, user_id,  action,  domain,  campaign_id,  cost, session
    
1420070400000,ip_1,user_5,clicked,facebook.com,campaign_6,139,session_98
1420070400864,ip_2,user_3,viewed,facebook.com,campaign_4,35,session_98
1420070401728,ip_8,user_8,clicked,youtube.com,campaign_12,115,session_92
1420070402592,ip_1,user_2,blocked,wikipedia.org,campaign_5,129,session_91</code></pre>
<p>Sample file located at <code>/data/click-stream/clickstream.json</code></p>
<h3 id="step-2-edit-source-file">STEP 2: Edit source file</h3>
<p>Go to the project root directory</p>
<pre><code>$    cd   ~/spark-labs/05-api</code></pre>
<p><strong>=&gt; Edit file : <code>src/main/scala/x/Clickstream.scala</code></strong><br />
<strong>=&gt; And fix the TODO items</strong></p>
<h3 id="step-2-compile-the-project">STEP 2: Compile the project</h3>
<p><strong>=&gt; Kick off a build</strong><br />
(This will take a few minutes for the first time you run it)</p>
<pre><code>$   sbt package
# to do a clean rebuild use
#  sbt clean package</code></pre>
<p>Make sure there are no errors and there is output in <code>target</code> dir.</p>
<pre><code>$  ls -l   target/scala-2.11</code></pre>
<p>You should see output like follows</p>
<pre><code>drwxr-xr-x  3 sujee  staff   102B Apr 16 09:59 classes/
-rw-r--r--  1 sujee  staff    13K Apr 16 09:59 testapp_2.11-1.0.jar</code></pre>
<p><code>testapp_2.11-1.0.jar</code> is our code compiled.</p>
<h3 id="step-3-start-spark-server">STEP 3: Start Spark Server</h3>
<pre><code>$  ~/spark/sbin/start-all.sh</code></pre>
<p><strong>=&gt; Check the Spark Server UI at port 8080.</strong><br />
<strong>=&gt; Note the Spark master URL.</strong></p>
<p><img src="../assets/images/4.1b.png" style="border: 5px solid grey; max-width:100%;"/></p>
<h3 id="step-4-submit-the-application">STEP 4: Submit the application</h3>
<p>Use the following command to submit the job</p>
<pre><code>$   ~/spark/bin/spark-submit --class &#39;x.Clickstream&#39; --master MASTER_URL  --driver-class-path  logging    target/scala-2.11/testapp_2.11-1.0.jar    &lt;files to process&gt;</code></pre>
<ul>
<li>MASTER URL : substitute your spark master url</li>
<li>for files you can try <code>data/click-stream/clickstream.csv</code></li>
</ul>
<p>Here is an example</p>
<pre><code>$   ~/spark/bin/spark-submit --class &#39;x.Clickstream&#39;  --driver-class-path  logging   target/scala-2.10/testapp_2.10-1.0.jar    &#39;../data/click-stream/clickstream.csv&#39;</code></pre>
<p><strong>=&gt; Watch the console output</strong></p>
<p>It may look like this</p>
<pre><code>### total clickstream records 20
### domain count :
Map(amazon.com -&gt; 2, funnyordie.com -&gt; 2, nytimes.com -&gt; 1, cnn.com -&gt; 2, youtube.com -&gt; 1, wikipedia.org -&gt; 2, facebook.com -&gt; 2, bbc.co.uk -&gt; 1, npr.org -&gt; 2, foxnews.com -&gt; 3, hulu.com -&gt; 2)
### top domains :
List((foxnews.com,3), (hulu.com,2), (npr.org,2), (facebook.com,2), (wikipedia.org,2), (cnn.com,2), (funnyordie.com,2), (amazon.com,2), (bbc.co.uk,1), (youtube.com,1), (nytimes.com,1))</code></pre>
<p>The lines starting with <code>###</code> are output from our program</p>
<h3 id="step-5-generate-some-clickstream-data">STEP 5 : Generate some clickstream data</h3>
<p>We have been testing with a small sample file of <code>data/click-stream/clickstream.csv</code> file. Now we are going to generate more data using a data-gen script located in <code>data/click-stream/gen-clickstream.py</code></p>
<pre><code>$  cd   ~/spark-labs/data/click-stream  #  cd to clickstream data dir
$  python gen-clickstream.py</code></pre>
<p>This script will generate some files with random clickstream data</p>
<pre><code>generating log  2015-01-01.csv
generating log  2015-01-02.csv
generating log  2015-01-03.csv
generating log  2015-01-04.csv
generating log  2015-01-05.csv
generating log  2015-01-06.csv
generating log  2015-01-07.csv
generating log  2015-01-08.csv
generating log  2015-01-09.csv
generating log  2015-01-10.csv</code></pre>
<h3 id="step-6-process-generated-clickstream-data">STEP 6 : Process Generated Clickstream Data</h3>
<pre><code>$    cd   ~/spark-labs/5-api
$    ~/spark/bin/spark-submit --class &#39;x.Clickstream&#39;   --driver-class-path  logging   target/scala-2.11/testapp_2.11-1.0.jar    &#39;../data/click-stream/*.csv&#39;</code></pre>
<p>Note: * we are including all the log files using a wild card <code>*.csv</code> * don’t forget the single quotes ’’</p>
<p><strong>=&gt; Note the time it took to process the entire logs. Compare it with the time to took process a single file</strong></p>
