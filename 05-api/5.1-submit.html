<p><link rel='stylesheet' href='../assets/css/main.css'/></p>
<p><a href="../README.html">&lt;&lt; back to main index</a></p>
<h1 id="lab-5.1-first-spark-job-submission">Lab 5.1 First Spark Job Submission</h1>
<h3 id="overview">Overview</h3>
<p>Submit a job for Spark</p>
<h3 id="depends-on">Depends On</h3>
<p>None</p>
<h3 id="run-time">Run time</h3>
<p>20-30 mins</p>
<h2 id="step-0-editing-files-on-vm">Step 0 : Editing Files on VM</h2>
<p>Please follow <a href="../edit-files.html">these instructions</a></p>
<h2 id="step-1-edit-source-file">STEP 1: Edit source file</h2>
<p>Go to the project root directory</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" data-line-number="1">    $    <span class="bu">cd</span> ~/spark-labs/05-api</a></code></pre></div>
<p><strong>=&gt; Edit file : <code>~/spark-labs/05-api/src/main/scala/x/ProcessFiles.scala</code></strong><br />
<strong>=&gt; And fix the TODO items</strong></p>
<h2 id="step-2-compile-the-project">STEP 2: Compile the project</h2>
<p>We will use <code>sbt</code> to build the project.</p>
<p><strong>=&gt; Inspect the <code>build.sbt</code> file</strong></p>
<p>The file will look follows:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode scala"><code class="sourceCode scala"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="co">// blank lines are important!</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">name := <span class="st">&quot;TestApp&quot;</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">version := <span class="st">&quot;1.0&quot;</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">scalaVersion := <span class="st">&quot;2.11.7&quot;</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8"></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">libraryDependencies ++= Seq(</a>
<a class="sourceLine" id="cb2-10" data-line-number="10">  <span class="st">&quot;org.apache.spark&quot;</span> %% <span class="st">&quot;spark-core&quot;</span> % <span class="st">&quot;2.4.3&quot;</span> % <span class="st">&quot;provided&quot;</span>,</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">  <span class="st">&quot;org.apache.spark&quot;</span> %% <span class="st">&quot;spark-sql&quot;</span> % <span class="st">&quot;2.4.3&quot;</span> % <span class="st">&quot;provided&quot;</span></a>
<a class="sourceLine" id="cb2-12" data-line-number="12">)</a>
<a class="sourceLine" id="cb2-13" data-line-number="13"></a>
<a class="sourceLine" id="cb2-14" data-line-number="14"><span class="co">// for accessing files from S3 or HDFS</span></a>
<a class="sourceLine" id="cb2-15" data-line-number="15">libraryDependencies += <span class="st">&quot;org.apache.hadoop&quot;</span> % <span class="st">&quot;hadoop-client&quot;</span> % <span class="st">&quot;2.7.0&quot;</span> <span class="fu">exclude</span>(<span class="st">&quot;com.google.guava&quot;</span>, <span class="st">&quot;guava&quot;</span>)</a>
<a class="sourceLine" id="cb2-16" data-line-number="16"></a></code></pre></div>
<p><strong>=&gt; Kick off a build</strong><br />
(This will take a few minutes for the first time you run it)</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb3-1" data-line-number="1">    <span class="co"># be in the project root level directory</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    $   <span class="bu">cd</span>   ~/spark-labs/05-api</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"></a>
<a class="sourceLine" id="cb3-4" data-line-number="4">    $   <span class="ex">sbt</span> package</a>
<a class="sourceLine" id="cb3-5" data-line-number="5"></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">    <span class="co"># to do a clean rebuild use</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    $  <span class="ex">sbt</span> clean package</a></code></pre></div>
<p>Make sure there are no errors and there is output in <code>target</code> dir.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" data-line-number="1">    $  <span class="fu">ls</span> -l   target/scala-2.11</a></code></pre></div>
<p>You should see output like follows</p>
<pre class="console"><code>    drwxr-xr-x  3 sujee  staff   102B Apr 16 09:59 classes/
    -rw-r--r--  1 sujee  staff    13K Apr 16 09:59 testapp_2.11-1.0.jar</code></pre>
<p><code>testapp_2.11-1.0.jar</code> is our code compiled.</p>
<h2 id="step-3-test-application-in-local-master-mode">STEP 3: Test Application in Local Master Mode</h2>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb6-1" data-line-number="1">    $  <span class="bu">cd</span>  ~/spark-labs/05-api</a>
<a class="sourceLine" id="cb6-2" data-line-number="2"></a>
<a class="sourceLine" id="cb6-3" data-line-number="3">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    README.html</a></code></pre></div>
<p><strong>==&gt; Checkout the Shell UI (4040)</strong></p>
<p><strong>==&gt; Hit Enter key to terminate the program</strong></p>
<p><strong>==&gt; Turn off the logs by sending logs by <code>2&gt; logs</code> </strong></p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb7-1" data-line-number="1">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    README.html  <span class="op">2&gt;</span> logs</a></code></pre></div>
<p><strong>==&gt; Try a few input files </strong></p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb8-1" data-line-number="1">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master local[*]  target/scala-2.11/testapp_2.11-1.0.jar    /data/text/twinkle/*  <span class="op">2&gt;</span> logs</a></code></pre></div>
<p>Now letâ€™s submit the application to Spark server</p>
<h2 id="step-4-start-spark-server">STEP 4: Start Spark Server</h2>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb9-1" data-line-number="1">    $  <span class="ex">~/spark/sbin/start-all.sh</span></a></code></pre></div>
<p><strong>=&gt; Check the Spark Server UI at port 8080.</strong><br />
<strong>=&gt; Note the Spark master URL.</strong></p>
<p><img src="../assets/images/4.1b.png" style="border: 5px solid grey; max-width:100%;"/></p>
<h2 id="step-5-submit-the-application">STEP 5: Submit the application</h2>
<p>Use the following command to submit the job</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb10-1" data-line-number="1">    $  <span class="bu">cd</span>  ~/spark-labs/05-api</a>
<a class="sourceLine" id="cb10-2" data-line-number="2"></a>
<a class="sourceLine" id="cb10-3" data-line-number="3">    <span class="co"># single README file</span></a>
<a class="sourceLine" id="cb10-4" data-line-number="4">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    README.html   <span class="op">2&gt;</span> logs</a>
<a class="sourceLine" id="cb10-5" data-line-number="5"></a>
<a class="sourceLine" id="cb10-6" data-line-number="6">    <span class="co"># multiple twinkle files</span></a>
<a class="sourceLine" id="cb10-7" data-line-number="7">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    /data/text/twinkle/*  <span class="op">2&gt;</span> logs</a></code></pre></div>
<ul>
<li>MASTER URL : substitute your spark master url</li>
<li>for files you can try <code>README.html</code></li>
</ul>
<p><strong>=&gt; Watch the console output</strong></p>
<p>It may look like this</p>
<pre class="console"><code>    15/04/15 15:41:51 INFO SparkUI: Started SparkUI at http://192.168.1.115:4040
    ...
    15/04/15 15:41:54 INFO DAGScheduler: Job 0 finished: count at ProcessFiles.scala:42, took 2.156893 s

    ### README.html : count (7) took 2,251.007000 ms</code></pre>
<p>The lines starting with <code>###</code> are output from our program</p>
<h2 id="step-6-configuring-logging">STEP 6: Configuring logging</h2>
<h4 id="to-quickly-turn-off-the-logging">To quickly turn off the logging:</h4>
<p>Redirect the logs as follows <code>2&gt; logs</code>.<br />
All logs will be sent to <code>logs</code> file.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb12-1" data-line-number="1">    $  <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master MASTER_URL  target/scala-2.11/testapp_2.11-1.0.jar    <span class="op">&lt;</span>files to process<span class="op">&gt;</span>    <span class="op">2&gt;</span>  logs</a></code></pre></div>
<h4 id="using-log4j-config">Using log4j config</h4>
<p>Spark by default logs at INFO level. While there is a lot of useful information there, it can be quite verbose.</p>
<p>We have a <code>logging/log4j.properties</code> file. Inspect this file</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb13-1" data-line-number="1">    $    <span class="fu">cat</span>   logging/log4j.properties</a></code></pre></div>
<p>The file has following contents</p>
<pre><code># Set everything to be logged to the console
log4j.rootCategory=WARN, console
log4j.appender.console=org.apache.log4j.ConsoleAppender
log4j.appender.console.target=System.err
log4j.appender.console.layout=org.apache.log4j.PatternLayout
log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

# Settings to quiet third party logs that are too verbose
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO

## set log levels for our loggers
## everything under &#39;x&#39; package
#log4j.logger.x=INFO
## specific file
#log4j.logger.x.ProcessFiles=DEBUG</code></pre>
<p>We provide <code>--driver-class-path logging/</code> to spark-submit to turn off logging</p>
<p>Here is an example</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb15-1" data-line-number="1">    $   <span class="ex">~/spark/bin/spark-submit</span> --class <span class="st">&#39;x.ProcessFiles&#39;</span> --master local[*]  --driver-class-path logging/  target/scala-2.11/testapp_2.11-1.0.jar    README.html</a></code></pre></div>
